{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ba675d-c73c-4eef-b5a1-beeaad2f08e2",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90d67ba-8630-4cc8-9cc8-658b4892ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from resources.plot_utils import plot_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807499b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run VAE_core.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e908c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28c82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in training data\n",
    "data_dir = \"/data/\"\n",
    "output_dir = data_dir + 'model/' \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# RFP profiles\n",
    "filename = os.path.join(data_dir, 'all_outputs.npy')\n",
    "data_array = np.load(filename)\n",
    "data_array = data_array.reshape([-1, 3, 201])\n",
    "RFP_data = data_array[:, 1, :].squeeze()\n",
    "print(f\"RFP profiles: {RFP_data.shape}\")\n",
    "\n",
    "# Normalize \n",
    "normalized_RFP = RFP_data / RFP_data.max(axis=1, keepdims=True)\n",
    "print(f\"Normalized RFP profiles: {normalized_RFP.shape}\")\n",
    "\n",
    "# Plot -- Create 100 panels (10x10), each showing a random data series\n",
    "num_panels = 100\n",
    "rows = 10\n",
    "cols = 10\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "fig.suptitle('Random RFP Data', fontsize=16)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        random_index = np.random.randint(normalized_RFP.shape[0])\n",
    "        axs[i, j].plot(normalized_RFP[random_index])\n",
    "        axs[i, j].set_ylim([0, 1])\n",
    "        axs[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c19bac-d998-44be-a29b-4bae66365169",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data = normalized_RFP # Use normalized RFP profile to train VAE\n",
    "seq_length = data.shape[1]\n",
    "batch_size = 32\n",
    "latent_dim = 16\n",
    "latent_channel=16\n",
    "\n",
    "alpha = 2e-5\n",
    "lr= 1e-3            \n",
    "min_lr = 5e-6      \n",
    "epochs = 1000\n",
    "gamma = 0.99\n",
    "weight_decay=1e-5\n",
    "\n",
    "# Split data\n",
    "data = torch.tensor(data).float().unsqueeze(1)\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "train_data, test_data, train_indices, test_indices = train_test_split(data, range(len(data)), test_size=0.1, random_state=25, shuffle=False)\n",
    "train_data, valid_data, train_indices, valid_indices = train_test_split(train_data, range(len(train_data)),test_size=0.1, random_state=25, shuffle=True)\n",
    "\n",
    "print(' --------------------------------------------------- ')\n",
    "print('Train data size: ', train_data.shape)\n",
    "print('Validation data size: ', valid_data.shape)\n",
    "print('Test data size: ', test_data.shape)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model creation, loss function, and optimizer\n",
    "model = VAE(seq_length, latent_dim, latent_channel)\n",
    "print(model)\n",
    "#Load previous model if it exists\n",
    "#model.load_state_dict(torch.load('VAE.pt'))\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'The model has {count_parameters(model):,} parameters')\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Early stopping \n",
    "best_test_loss = np.inf  \n",
    "epochs_no_improve = 0  # Counter for epochs since the test loss last improved\n",
    "patience = 30 # Patience for early stopping\n",
    "\n",
    "# Warm up\n",
    "warmup_epochs = 10\n",
    "def warmup_scheduler(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_scheduler)\n",
    "scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "# Training loop\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_VAE(model, train_loader, optimizer, criterion, alpha, device)\n",
    "    valid_loss = validate_VAE(model, valid_loader, criterion, alpha, device)\n",
    "    test_loss = test_VAE(model, test_loader, criterion, alpha, device)\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "\n",
    "    # Clamp minimum learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = max(param_group['lr'], min_lr)\n",
    "\n",
    "    # Print loss\n",
    "    if (epoch + 1) % 5 == 0: # every 5 epochs\n",
    "        print('Epoch: {} Train: {:.7f}, Valid: {:.7f}, Test: {:.7f}, Lr:{:.8f}'.format(epoch + 1, train_loss_history[epoch], valid_loss_history[epoch], test_loss_history[epoch], param_group['lr']))\n",
    "    \n",
    "    # Update learning rate\n",
    "    if epoch < warmup_epochs:\n",
    "        scheduler1.step()\n",
    "    else:\n",
    "        scheduler2.step()\n",
    "\n",
    "    # Check for early stopping\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        epochs_no_improve = 0  # Reset the counter\n",
    "    else:\n",
    "        epochs_no_improve += 1  # Increment the counter\n",
    "\n",
    "    if epochs_no_improve == patience:\n",
    "        print('Early stopping!')\n",
    "        break  # Exit the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss history\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.semilogy(train_loss_history, label='Training')\n",
    "plt.semilogy(valid_loss_history, label='Validation')\n",
    "plt.semilogy(test_loss_history, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot R2\n",
    "model.eval()\n",
    "train_data_short = train_data[0: len(test_data)]\n",
    "test_data_short = test_data\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_data_short = train_data_short.to(device)\n",
    "    test_data_short = test_data_short.to(device)\n",
    "\n",
    "    train_pred, _, _= model(train_data_short)\n",
    "    test_pred, _, _ = model(test_data_short)\n",
    "    \n",
    "train_data_short = train_data_short.squeeze(1).cpu().numpy()\n",
    "test_data_short = test_data_short.squeeze(1).cpu().numpy()\n",
    "train_pred = train_pred.squeeze(1).cpu().numpy()\n",
    "test_pred = test_pred.squeeze(1).cpu().numpy()\n",
    "\n",
    "filename = output_dir + 'VAE_train_R2.png'\n",
    "plot_R2(train_data_short, train_pred, filename)\n",
    "filename = output_dir + 'VAE_test_R2.png'\n",
    "plot_R2(test_data_short, test_pred, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    axs[0, i].plot(train_data_short[i].squeeze(), label='Original', color='blue')\n",
    "    axs[0, i].plot(train_pred[i], label='Reconstructed', color='orange')\n",
    "    axs[0, i].set_title(f'Train {i + 1}')\n",
    "    # axs[0, i].legend()\n",
    "\n",
    "for i in range(5):\n",
    "    axs[1, i].plot(test_data_short[i].squeeze(), label='Original', color='blue')\n",
    "    axs[1, i].plot(test_pred[i], label='Reconstructed', color='orange')\n",
    "    axs[1, i].set_title(f'Test {i + 1}')\n",
    "    # axs[1, i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca14346",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12309a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "filename = output_dir + 'VAE.pt'\n",
    "print(filename)\n",
    "torch.save(model.state_dict(), filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
