{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e310eb1-d039-4e58-bcb7-6af270de3b22",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664e68c9-f5e9-4646-a490-ed03cab1e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR, ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from resources.plot_utils import plot_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0e44a-1e4c-4cda-bf34-16bb970ad007",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517081e8-9106-4ea4-a7dc-9a91bf92f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data\n",
    "data_directory = \"./all_data/\"\n",
    "output_dir = data_directory + 'model/'\n",
    "\n",
    "# Load RFP profiles\n",
    "data_array = np.load(os.path.join(data_directory, 'all_outputs.npy'))\n",
    "data_array = data_array.reshape([-1, 3, 201])\n",
    "RFP_data = data_array[:, 1, :].squeeze()\n",
    "# Normalize \n",
    "normalized_RFP = RFP_data / RFP_data.max(axis=1, keepdims=True)\n",
    "normalized_RFP = normalized_RFP.reshape([-1, 1, 201])\n",
    "\n",
    "# Parameters \n",
    "filename = data_directory + 'all_params.npy' # original scale\n",
    "params_array = np.load(filename)\n",
    "scaling_ranges = {\n",
    "    'DC': [0.5e-3, 12.5e-2],\n",
    "    'aC': [0.1, 1],\n",
    "    'aA': [100, 100000],\n",
    "    'aT': [10, 8000],\n",
    "    'aL': [5, 500],\n",
    "    'dA': [0.001, 0.1],\n",
    "    'dT': [3, 300],\n",
    "    'dL': [0.144, 14.4],\n",
    "    'alpha': [1, 5],\n",
    "    'beta':  [2, 2000],\n",
    "    'Kphi':  [1, 10],\n",
    "    'N0':  [200000, 5000000]\n",
    "}\n",
    "scaling_options = ['exp','linear','exp','exp','exp',\n",
    "                   'exp','exp','exp','linear','exp',\n",
    "                   'linear','linear']\n",
    "all_params = ['DC', 'DN', 'DA', 'DB', 'aC','aA', 'aB', 'aT', 'aL', 'bN','dA', 'dB', 'dT', 'dL', 'k1', \n",
    "              'k2', 'KN', 'KP', 'KT', 'KA', 'KB', 'alpha','beta', 'Cmax', 'a', 'b', 'm', 'n', 'Kphi', 'l', \n",
    "              'N0', 'G1','G2','G3','G4','G5','G6','G7','G8','G9','G10','G11','G12', 'G13','G14',\n",
    "             'G15','G16','G17','G18', 'G19', 'alpha_p','beta_p', 'seeding_v']\n",
    "sceening_params = ['DC',  'aC', 'aA', 'aT', 'aL', 'dA','dT', 'dL', 'alpha','beta','Kphi', 'N0']\n",
    "selected_param_idx = [all_params.index(param) for param in sceening_params]\n",
    "params_array = params_array[:, selected_param_idx]\n",
    "\n",
    "# Pattern types\n",
    "pattern_types_array = np.load(os.path.join(data_directory, 'all_types.npy'))\n",
    "pattern_types_array = pattern_types_array[:, 1]\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print(f\"RFP profiles: {normalized_RFP.shape}\")\n",
    "print(f\"Parameters: {params_array.shape}\")\n",
    "print(f\"Pattern types:  {pattern_types_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13985fa-1278-4e16-a506-d1030184c3bf",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86579e14-6449-4059-b3be-cd2d1be3464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and check device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Set up hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = data_array.shape[2] # dimension of the 1D profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8495f6d-5cec-4d3e-a8c2-447b397a92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize params\n",
    "scaling_ranges = {\n",
    "    'DC': [0.5e-3, 12.5e-2],\n",
    "    'aC': [0.1, 1],\n",
    "    'aA': [100, 100000],\n",
    "    'aT': [10, 8000],\n",
    "    'aL': [5, 500],\n",
    "    'dA': [0.001, 0.1],\n",
    "    'dT': [3, 300],\n",
    "    'dL': [0.144, 14.4],\n",
    "    'alpha': [1, 5],\n",
    "    'beta':  [2, 2000],\n",
    "    'Kphi':  [1, 10],\n",
    "    'N0':  [200000, 5000000]\n",
    "}\n",
    "scaling_options = ['exp','linear','exp','exp','exp', 'exp','exp','exp','linear','exp','linear','linear']\n",
    "\n",
    "def scale_feature(value, min_val, max_val, option):\n",
    "    if option == \"linear\":\n",
    "        return (value - min_val) / (max_val - min_val)\n",
    "    elif option == \"exp\":\n",
    "        return (np.log(value) - np.log(min_val)) / (np.log(max_val) - np.log(min_val))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaling option: {option}\")\n",
    "\n",
    "def scale_dataset(dataset, ranges, opt):\n",
    "    scaled_dataset = np.zeros_like(dataset)\n",
    "    for i, (key, range_vals) in enumerate(ranges.items()):\n",
    "        min_val, max_val = range_vals\n",
    "        scaling_option = opt[i]\n",
    "        scaled_dataset[:, i] = [scale_feature(val, min_val, max_val, scaling_option) for val in dataset[:, i]]\n",
    "    return scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a589aa4-a0f3-4d28-8302-db62b510192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run MLP_VAE_core.ipynb\n",
    "%run VAE_core.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ac6ad-82fb-4ee0-95b9-590f08fab8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_list =[0.1, 0.25, 0.5, 0.9, 0.95]\n",
    "latent_dim_list = [8, 16, 32, 64]\n",
    "num_models = len(latent_dim_list)\n",
    "latent_channel = 16\n",
    "\n",
    "model_list = []\n",
    "vae_r2_traing_list = []\n",
    "vae_r2_test_list = []\n",
    "mlp_r2_traing_list = []\n",
    "mlp_r2_test_list = []\n",
    "# train_set_size = []\n",
    "\n",
    "# Get a test set first, do not update it in the for loop\n",
    "ratio = 0.1\n",
    "data_array = torch.tensor(data_array).float()\n",
    "train_data_all, test_data, train_indices_all, test_indices = train_test_split(normalized_RFP, range(normalized_RFP.shape[0]), test_size=ratio, random_state=25)\n",
    "train_params_all, test_params, train_indices_all, test_indices = train_test_split(params_array, range(params_array.shape[0]), test_size=ratio, random_state=25)\n",
    "train_labels_all, test_labels, train_indices_all, test_indices = train_test_split(pattern_types_array, range(pattern_types_array.shape[0]), test_size=ratio, random_state=25)\n",
    "\n",
    "train_data_all = train_data_all[:, 0, :].reshape([-1, 1, 201])\n",
    "test_data = test_data[:, 0, :].reshape([-1, 1, 201])\n",
    "\n",
    "print('Test set size: ', len(test_data))\n",
    "print('Complete train set size: ', len(train_data_all))\n",
    "\n",
    "test_data = torch.tensor(test_data).float()\n",
    "train_data_all = torch.tensor(train_data_all).float()\n",
    "\n",
    "\n",
    "for i in range(0, num_models):\n",
    "    \n",
    "    latent_dim = latent_dim_list[i]\n",
    "    ratio = 0.1\n",
    "\n",
    "    print('----------------------  latent dim: ', latent_dim, '----------------------') \n",
    "    \n",
    "    # Split train and validation datasets\n",
    "    train_data, valid_data, train_indices, valid_indices = train_test_split(train_data_all, range(train_data_all.shape[0]), test_size=ratio, random_state=15)\n",
    "    train_params, valid_params, train_indices, valid_indices = train_test_split(train_params_all, range(train_params_all.shape[0]), test_size=ratio, random_state=15)\n",
    "    train_labels, valid_labels, train_indices, valid_indices = train_test_split(train_labels_all, range(train_labels_all.shape[0]), test_size=ratio, random_state=15)\n",
    "\n",
    "    # Normalize parameters\n",
    "    train_params = scale_dataset(train_params, scaling_ranges, scaling_options)\n",
    "    valid_params = scale_dataset(valid_params, scaling_ranges, scaling_options)\n",
    "    test_params = scale_dataset(test_params, scaling_ranges, scaling_options)\n",
    "\n",
    "    # Train set dataloader\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    # Valid set loader\n",
    "    valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "    # Test set loader\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    print('Train set size: ', len(train_data))\n",
    "    print('Valid set size: ', len(valid_data))\n",
    "    print('Test set size: ', len(test_data))\n",
    "\n",
    "    print('------------------- VAE training ---------------------------')\n",
    "    # Initiate VAE\n",
    "    vae = VAE(seq_length, latent_dim, latent_channel)\n",
    "\n",
    "    # # Load VAE if exist\n",
    "    # model_path = os.path.join(output_dir,'screening_VAE_latent_' + str(latent_dim) + '.pt')\n",
    "    # print(model_path)\n",
    "    # vae.load_state_dict(torch.load(model_path))\n",
    "    # vae.eval() \n",
    "\n",
    "    # Send model to device\n",
    "    vae = vae.to(device)\n",
    "\n",
    "    # Training setup\n",
    "    alpha = 2e-5\n",
    "    lr= 3e-5            \n",
    "    min_lr = 5e-6      \n",
    "    epochs = 1000\n",
    "    gamma = 0.99\n",
    "    weight_decay=1e-5\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Early stopping \n",
    "    best_valid_loss = np.inf  \n",
    "    epochs_no_improve = 0  # Counter for epochs since the test loss last improved\n",
    "    patience = 30 # Patience for early stopping\n",
    "\n",
    "    # Warm up\n",
    "    warmup_epochs = 10\n",
    "    def warmup_scheduler(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_scheduler)\n",
    "    scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    # Train \n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_VAE(vae, train_loader, optimizer, criterion, alpha, device)\n",
    "        valid_loss = validate_VAE(vae, valid_loader, criterion, alpha, device)\n",
    "        test_loss = test_VAE(vae, test_loader, criterion, alpha, device)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        # Clamp minimum learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = max(param_group['lr'], min_lr)\n",
    "\n",
    "        # Print loss\n",
    "        if (epoch) % 5 == 0: # every 5 epochs\n",
    "            print('Epoch: {} Train: {:.7f}, Valid: {:.7f}, Test: {:.7f}, Lr:{:.8f}'.format(epoch + 1, train_loss_history[epoch], valid_loss_history[epoch], test_loss_history[epoch], param_group['lr']))\n",
    "\n",
    "        # Update learning rate\n",
    "        if epoch < warmup_epochs:\n",
    "            scheduler1.step()\n",
    "        else:\n",
    "            scheduler2.step()\n",
    "        scheduler2.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0  # Reset the counter\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment the counter\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            break  # Exit the loop\n",
    "   \n",
    "    # Save VAE\n",
    "    model_path = os.path.join(output_dir,'screening_VAE_latent_' + str(latent_dim) + '.pt')\n",
    "    print('model path: ', model_path)\n",
    "    torch.save(vae.state_dict(), model_path)\n",
    "    \n",
    "    \n",
    "    print('------------------- Evaluate VAE performance ---------------------------')\n",
    "    # Calculate trained VAE accuracy\n",
    "    train_data = train_data.cpu()\n",
    "\n",
    "    train_data_ori = torch.tensor(train_data[0:1000], dtype=torch.float32).to(device)\n",
    "    test_data_ori = torch.tensor(test_data[0:1000], dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_pred, _, _ = vae(train_data_ori)\n",
    "        test_pred, _, _ = vae(test_data_ori)\n",
    "\n",
    "    # Squeeze the output to match the original data dimension\n",
    "    train_pred = train_pred.squeeze(1).cpu().numpy()\n",
    "    test_pred = test_pred.squeeze(1).cpu().numpy()\n",
    "    train_data_ori = train_data_ori.cpu().numpy().squeeze(1)\n",
    "    test_data_ori = test_data_ori.cpu().numpy().squeeze(1)\n",
    "\n",
    "    r2_train = r2_score(train_data_ori.flatten(), train_pred.flatten())\n",
    "    r2_test = r2_score(test_data_ori.flatten(), test_pred.flatten())\n",
    "    \n",
    "    print('******** VAE ********')\n",
    "    print('R2 train: ', r2_train)\n",
    "    print('R2 test: ', r2_test)\n",
    "    vae_r2_traing_list.append(r2_train)\n",
    "    vae_r2_test_list.append(r2_test)\n",
    "\n",
    "    print('------------------- MLP training ---------------------------')\n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset_with_type(torch.tensor(train_params, dtype=torch.float32), \n",
    "                              torch.tensor(train_data, dtype=torch.float32),\n",
    "                              torch.tensor(train_labels, dtype=torch.float32))\n",
    "    valid_dataset = CustomDataset_with_type(torch.tensor(valid_params, dtype=torch.float32), \n",
    "                                 torch.tensor(valid_data, dtype=torch.float32),\n",
    "                                 torch.tensor(valid_labels, dtype=torch.float32))\n",
    "    test_dataset = CustomDataset_with_type(torch.tensor(test_params, dtype=torch.float32), \n",
    "                                 torch.tensor(test_data, dtype=torch.float32),\n",
    "                                 torch.tensor(test_labels, dtype=torch.float32))\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    print('Traning data points:', len(train_dataset))\n",
    "    print('Valid data points:', len(valid_dataset))\n",
    "    print('Test data points:', len(test_dataset))\n",
    "\n",
    "    # Initiate MLP\n",
    "    input_dim = len(train_params[0,:])\n",
    "    model = CombinedModel(input_dim, latent_dim, vae.decoder, 42)\n",
    "    \n",
    "    # # Load model if exist\n",
    "    # model_path = os.path.join(output_dir,'screening_MLP_latent_' + str(latent_dim) + '.pt')\n",
    "    # print(model_path)\n",
    "    # model.load_state_dict(torch.load(model_path))\n",
    "    # model.eval() \n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    # Train MLP\n",
    "    lr = 1e-4            \n",
    "    min_lr = 1e-7      \n",
    "    epochs = 1000\n",
    "    gamma = 0.99\n",
    "    weight_decay = 1e-5\n",
    "    alpha = 2e-5 #2e-5 not used\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = MSELoss()\n",
    "    optimizer = Adam(model.mlp.parameters(), lr= lr, weight_decay=weight_decay)  # Only train MLP parameters\n",
    "\n",
    "    #  Warmup \n",
    "    warmup_epochs = 8\n",
    "\n",
    "    # Scheduler \n",
    "    scheduler1 = LambdaLR(optimizer, lr_lambda=warmup_scheduler)\n",
    "    scheduler2 = ExponentialLR(optimizer, gamma=0.995)\n",
    "\n",
    "    # Early stopping setup\n",
    "    best_valid_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    patience = 30\n",
    "    \n",
    "    # Training loop\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_combined(model, train_loader, optimizer, criterion, alpha, device)\n",
    "        valid_loss = validate_combined(model, valid_loader, criterion, alpha, device)\n",
    "        test_loss = test_combined(model, test_loader, criterion, alpha, device)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        # Clamp minimum learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = max(param_group['lr'], min_lr)\n",
    "\n",
    "        # Print loss\n",
    "        if epoch % 5 == 0: # every 5 epochs\n",
    "            print('Epoch: {} Train: {:.7f}, Valid: {:.7f}, Test: {:.7f}, Lr:{:.8f}'.format(epoch + 1, train_loss_history[epoch], valid_loss_history[epoch], test_loss_history[epoch], param_group['lr']))\n",
    "\n",
    "        # Update learning rate\n",
    "        if epoch < warmup_epochs:\n",
    "            scheduler1.step()\n",
    "        else:\n",
    "            scheduler2.step()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0  # Reset the counter\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment the counter\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            break  # Exit the loop\n",
    "        if valid_loss < 0.0001:\n",
    "            break\n",
    "\n",
    "    # Save MLP\n",
    "    model_path = os.path.join(output_dir,'screening_MLP_latent_' + str(latent_dim) + '.pt')\n",
    "    print('model path: ', model_path)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    \n",
    "    print('------------------- Evaluate MLP - VAE performance ---------------------------')\n",
    "\n",
    "    # Calculate trained MLP accuracies\n",
    "    model.eval()\n",
    "    train_pred = []\n",
    "    train_ori = []\n",
    "    test_pred = []\n",
    "    test_ori = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for params, data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            params = params.to(device)\n",
    "            reconstruction, mean, logvar = model(params)\n",
    "            train_pred.append(reconstruction.cpu().numpy())\n",
    "            train_ori.append(data.cpu().numpy())\n",
    "\n",
    "        for params, data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            params = params.to(device)\n",
    "            reconstruction, mean, logvar = model(params)\n",
    "            test_pred.append(reconstruction.cpu().numpy())\n",
    "            test_ori.append(data.cpu().numpy())\n",
    "\n",
    "    # Concatenate\n",
    "    train_pred = np.concatenate(train_pred)\n",
    "    train_ori = np.concatenate(train_ori)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_ori = np.concatenate(test_ori)\n",
    "    \n",
    "    \n",
    "    filename = output_dir + 'MLP_VAE_train_R2.png'\n",
    "    plot_R2(train_ori, train_pred, filename)\n",
    "    filename = output_dir + 'MLP_VAE_test_R2.png'\n",
    "    plot_R2(test_ori, test_pred, filename)\n",
    "    print(filename)\n",
    "    \n",
    "    # Squeeze the output to match the original data dimension\n",
    "    train_pred = train_pred.squeeze(1)\n",
    "    test_pred = test_pred.squeeze(1)\n",
    "    train_ori = train_ori.squeeze(1)\n",
    "    test_ori = test_ori.squeeze(1)\n",
    "\n",
    "    r2_train = r2_score(train_ori.flatten(), train_pred.flatten())\n",
    "    r2_test = r2_score(test_ori.flatten(), test_pred.flatten())\n",
    "    \n",
    "    print('******** MLP ********')\n",
    "    print('R2 train: ', r2_train)\n",
    "    print('R2 test: ', r2_test)\n",
    "    \n",
    "    mlp_r2_traing_list.append(r2_train)\n",
    "    mlp_r2_test_list.append(r2_test)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b05ea-f7ce-4dbd-8ec3-9ff8387397bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "\n",
    "# Plotting the data\n",
    "axs[0].plot(latent_dim_list, vae_r2_test_list)\n",
    "axs[1].plot(latent_dim_list, mlp_r2_test_list)\n",
    "\n",
    "# Setting titles for each subplot\n",
    "axs[0].set_title('VAE ')\n",
    "axs[1].set_title('MLP - VAE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
