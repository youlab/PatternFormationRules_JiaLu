{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69003028-0e8c-498d-b80d-5d0f808059ae",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6ffb7e9c-3671-4dbb-af95-5e30937bc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy.signal as signal\n",
    "from scipy.io import loadmat\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a74a5-71b4-4f9c-8e5a-9d8f5a5673d0",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f9787a02-18e0-4d28-8177-7a3e1a559f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \".data/\"\n",
    "output_path = root_path \n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Individual folders\n",
    "dataset_dir_1 = root_path \n",
    "seeding_v_1 = 0.1 # seeding volume\n",
    "par_num = 53 # PDE parameter number\n",
    "\n",
    "# All folders to process\n",
    "folder_info = [(dataset_dir_1, seeding_v_1, par_num)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57966823-b482-4d3f-9c66-42983d93893e",
   "metadata": {},
   "source": [
    "# Load and process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f4f7e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "757c44dd-1953-4794-ac97-b355199ca9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data from .mat files\n",
    "def load_and_process_data(folder_name, file, seeding_v):\n",
    "\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "\n",
    "    # Return None for both outputs and params if the file is empty\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        return None, None  \n",
    "    \n",
    "    # Load .mat file\n",
    "    mat_contents = loadmat(file_path)\n",
    "    param = mat_contents['param']\n",
    "\n",
    "    # Shorten profile\n",
    "    length = 1001\n",
    "    skip = 5 \n",
    "    \n",
    "    # Get the last line of hist_Ce, hist_Ly, and hist_T\n",
    "    last_line_Ce = param['hist_Ce'][0, 0][0:length:skip, -1]\n",
    "    total_RFP = param['hist_RFP'][0, 0][0:length:skip, -1]   \n",
    "    total_CFP = param['hist_CFP'][0, 0][0:length:skip, -1]\n",
    "    \n",
    "    # Filter off negative values\n",
    "    last_line_Ce[last_line_Ce < 0] = 0\n",
    "    total_RFP[total_RFP < 0] = 0\n",
    "    total_CFP[total_CFP < 0] = 0\n",
    "    \n",
    "    # Set NaN values to 0\n",
    "    last_line_Ce[np.isnan(last_line_Ce)] = 0\n",
    "    total_RFP[np.isnan(total_RFP)] = 0\n",
    "    total_CFP[np.isnan(total_CFP)] = 0\n",
    "    \n",
    "    # Construct full 1D profiles\n",
    "    flipped_Ce = np.flip(last_line_Ce)\n",
    "    flipped_RFP = np.flip(total_RFP)\n",
    "    flipped_CFP = np.flip(total_CFP)\n",
    "    \n",
    "    concat_Ce = np.concatenate((flipped_Ce, last_line_Ce))\n",
    "    concat_RFP = np.concatenate((flipped_RFP, total_RFP))\n",
    "    concat_CFP = np.concatenate((flipped_CFP, total_CFP))\n",
    "\n",
    "    # Normalize               \n",
    "    data_C_norm = (concat_Ce - np.min(concat_Ce)) / (np.max(concat_Ce) - np.min(concat_Ce))\n",
    "    data_RFP_norm = (concat_RFP - np.min(concat_RFP)) / (np.max(concat_RFP) - np.min(concat_RFP))\n",
    "    data_CFP_norm = (concat_CFP - np.min(concat_CFP)) / (np.max(concat_CFP) - np.min(concat_CFP))\n",
    "\n",
    "    # Get pattern type of the originals\n",
    "    peaks_Ce, _ = signal.find_peaks(data_C_norm, distance=5, width = 3, height = 0.1*np.max(data_C_norm), prominence=0.03*np.max(data_C_norm))\n",
    "    peaks_RFP, _ = signal.find_peaks(data_RFP_norm, distance=5, width = 3, height = 0.03*np.max(data_RFP_norm), prominence=0.03*np.max(data_RFP_norm))\n",
    "    peaks_CFP, _ = signal.find_peaks(data_CFP_norm, distance=5, width = 3, height = 0.03*np.max(data_CFP_norm), prominence=0.03*np.max(data_CFP_norm))\n",
    "\n",
    "    params = np.column_stack([\n",
    "        param['DC'][0,0][0][0], param['DN'][0,0][0][0], param['DA'][0,0][0][0], param['DB'][0,0][0][0],\n",
    "        param['aC'][0,0][0][0], param['aA'][0,0][0][0], param['aB'][0,0][0][0], param['aT'][0,0][0][0],\n",
    "        param['aL'][0,0][0][0], param['bN'][0,0][0][0], param['dA'][0,0][0][0], param['dB'][0,0][0][0],\n",
    "        param['dT'][0,0][0][0], param['dL'][0,0][0][0], param['k1'][0,0][0][0], param['k2'][0,0][0][0],\n",
    "        param['KN'][0,0][0][0], param['KP'][0,0][0][0], param['KT'][0,0][0][0], param['KA'][0,0][0][0],\n",
    "        param['KB'][0,0][0][0], param['alpha'][0,0][0][0], param['beta'][0,0][0][0], param['Cmax'][0,0][0][0],\n",
    "        param['a'][0,0][0][0], param['b'][0,0][0][0], param['m'][0,0][0][0], param['n'][0,0][0][0],\n",
    "        param['Kphi'][0,0][0][0], param['l'][0,0][0][0], param['N0'][0,0][0][0], param['G1'][0,0][0][0],\n",
    "        param['G2'][0,0][0][0], param['G3'][0,0][0][0], param['G4'][0,0][0][0], param['G5'][0,0][0][0],\n",
    "        param['G6'][0,0][0][0], param['G7'][0,0][0][0], param['G8'][0,0][0][0], param['G9'][0,0][0][0],\n",
    "        param['G10'][0,0][0][0], param['G11'][0,0][0][0], param['G12'][0,0][0][0], param['G13'][0,0][0][0],\n",
    "        param['G14'][0,0][0][0], param['G15'][0,0][0][0], param['G16'][0,0][0][0], param['G17'][0,0][0][0],\n",
    "        param['G18'][0,0][0][0], param['G19'][0,0][0][0], param['alpha_p'][0,0][0][0], param['beta_p'][0,0][0][0],\n",
    "        seeding_v, #param['taskID'][0,0][0][0]  \n",
    "    ])\n",
    "\n",
    "    pattern_types = np.column_stack([len(peaks_Ce), len(peaks_RFP), len(peaks_CFP)])\n",
    "\n",
    "    return [last_line_Ce, total_RFP, total_CFP], params, pattern_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7bcec36f-5ef6-4b75-8f26-1b4070a63456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_chunk(folder_name, files_chunk, seeding_v):\n",
    "\n",
    "    # Parallelize file chunks\n",
    "    results = Parallel(n_jobs=-1)(delayed(load_and_process_data)(folder_name, file, seeding_v) for file in files_chunk)\n",
    "    \n",
    "    # Filter out results where outputs or params is None (e.g. empty files)\n",
    "    results = [res for res in results if res[0] is not None and res[1] is not None]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8dc40db6-f870-4978-8bc9-d3257baa8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process .mat files in each folder\n",
    "def process_folder(folder_name, seeding_v):\n",
    "\n",
    "    folder_path = os.path.join(root_path, folder_name)\n",
    "    \n",
    "    # Get all .mat files\n",
    "    files = [file for file in os.listdir(folder_path) if file.endswith(\".mat\")]\n",
    "\n",
    "    # Initialize variables\n",
    "    temp_files = []\n",
    "    CHUNK_SIZE = 100  # Define the size of each chunk\n",
    "\n",
    "    # Process files in chunks\n",
    "    for i in tqdm(range(0, len(files), CHUNK_SIZE)):\n",
    "\n",
    "        # grab chuck of files\n",
    "        files_chunk = [os.path.join(folder_path, file) for file in files[i:i + CHUNK_SIZE]]\n",
    "\n",
    "        # Process each file chunk, skip unreadable files or files with no output\n",
    "        chunk_results = process_file_chunk(folder_name, files_chunk, seeding_v)  \n",
    "\n",
    "        # Filter out None results from chunk_results\n",
    "        chunk_results = [result for result in chunk_results if result is not None and result[0] is not None]\n",
    "\n",
    "        # If no valid results in the chunk, skip to the next chunk\n",
    "        if not chunk_results:\n",
    "            continue\n",
    "\n",
    "        all_outputs = np.array([res[0] for res in chunk_results])\n",
    "        all_params = np.array([res[1] for res in chunk_results])\n",
    "        all_types = np.array([res[2] for res in chunk_results])\n",
    "        print(all_outputs.shape)\n",
    "        \n",
    "        # Determine the maximum shape in each dimension\n",
    "        max_output_shape = tuple(max(sizes) for sizes in zip(*[output.shape for output in all_outputs]))\n",
    "        max_param_shape = tuple(max(sizes) for sizes in zip(*[param.shape for param in all_params]))\n",
    "        max_type_shape = tuple(max(sizes) for sizes in zip(*[type_.shape for type_ in all_types]))\n",
    "\n",
    "        # Pad the arrays to ensure consistent shapes\n",
    "        all_outputs_padded = np.array([np.pad(output, [(0, m - s) for s, m in zip(output.shape, max_output_shape)], mode='constant') for output in all_outputs])\n",
    "        all_params_padded = np.array([np.pad(param, [(0, m - s) for s, m in zip(param.shape, max_param_shape)], mode='constant') for param in all_params])\n",
    "        all_types_padded = np.array([np.pad(type_, [(0, m - s) for s, m in zip(type_.shape, max_type_shape)], mode='constant') for type_ in all_types])\n",
    "\n",
    "        # Save as temp files\n",
    "        temp_filename = f\"{folder_name}_temp_{i // CHUNK_SIZE + 1}.npz\"\n",
    "        temp_filename = os.path.join(folder_path, temp_filename)\n",
    "        print(temp_filename)\n",
    "        \n",
    "        np.savez(temp_filename,\n",
    "                    all_outputs=all_outputs_padded,\n",
    "                    all_params=all_params_padded.squeeze(),\n",
    "                    all_types=all_types_padded.squeeze())\n",
    "        temp_files.append(temp_filename)\n",
    "\n",
    "    return temp_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "07046f0e-0407-4f62-8c9f-41ecb42b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together all temp file data\n",
    "def gether_all_data(folder_name):\n",
    "    \n",
    "    # Load all temporary files and concatenate the results\n",
    "    all_outputs_combined = []\n",
    "    all_params_combined = []\n",
    "    all_types_combined = []\n",
    "    \n",
    "    # grab temp files\n",
    "    temp_files = [temp_file for temp_file in os.listdir(folder_name) if 'temp' in temp_file]\n",
    "    \n",
    "    for temp_file in temp_files:\n",
    "        temp_file_path = os.path.join(folder_name, temp_file)\n",
    "        \n",
    "        with np.load(temp_file_path, allow_pickle=True) as data:\n",
    "            \n",
    "            all_outputs = data['all_outputs']\n",
    "            all_params = data['all_params']\n",
    "            all_types = data['all_types']\n",
    "            print(np.array(all_outputs).shape)\n",
    "            \n",
    "            if len(np.array(all_outputs).shape) == 3:                 \n",
    "                all_outputs_combined.append(all_outputs)\n",
    "                all_params_combined.append(all_params)\n",
    "                all_types_combined.append(all_types)\n",
    "         \n",
    "    all_outputs_combined = np.vstack(all_outputs_combined)\n",
    "    all_params_combined = np.vstack(all_params_combined)\n",
    "    all_types_combined = np.vstack(all_types_combined)\n",
    "    \n",
    "    # Check final dataset size\n",
    "    print(all_outputs_combined.shape)\n",
    "    print(all_params_combined.shape)\n",
    "    print(all_types_combined.shape)\n",
    "    \n",
    "    # Save the final concatenated results specific to this folder -- in each dataset's dir\n",
    "    np.save(os.path.join(folder_name, f\"all_outputs_{os.path.basename(folder_name)}.npy\"), all_outputs_combined)\n",
    "    np.save(os.path.join(folder_name, f'all_params_{os.path.basename(folder_name)}.npy'), all_params_combined)\n",
    "    np.save(os.path.join(folder_name, f'all_types_{os.path.basename(folder_name)}.npy'), all_types_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a5d5c9e3-8b66-4e16-964d-972d7867f469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate data from each folder, and save as one single dataset\n",
    "def gather_final_dataset(folder_info, output_path):\n",
    "    \n",
    "    all_outputs_combined = []\n",
    "    all_params_combined = []\n",
    "    all_types_combined = []\n",
    "    \n",
    "    all_output_files = []\n",
    "    all_params_files = []\n",
    "    all_types_files = []\n",
    "    \n",
    "    for folder_path, _, _ in folder_info:\n",
    "        print('Processing ... ', folder_path)\n",
    "        for file in os.listdir(folder_path):\n",
    "\n",
    "            if \"all_outputs_\" in file and file.endswith(\".npy\"):\n",
    "                all_output_files.append(file)\n",
    "                file_name = os.path.join(folder_path, file)\n",
    "                data = np.load(file_name) \n",
    "                print(data.shape)\n",
    "                all_outputs_combined.append(data)             \n",
    "\n",
    "            if \"all_params_\" in file and file.endswith(\".npy\"):\n",
    "                all_params_files.append(file)\n",
    "                file_name = os.path.join(folder_path, file)\n",
    "                data = np.load(file_name)\n",
    "                print(data.shape)\n",
    "                all_params_combined.append(data)\n",
    "\n",
    "            if \"all_types_\" in file and file.endswith(\".npy\"):\n",
    "                all_types_files.append(file)\n",
    "                file_name = os.path.join(folder_path, file)\n",
    "                data = np.load(file_name)\n",
    "                print(data.shape)\n",
    "                all_types_combined.append(data)\n",
    "   \n",
    "    all_outputs_combined = np.vstack(all_outputs_combined)\n",
    "    all_params_combined = np.vstack(all_params_combined)\n",
    "    all_types_combined = np.vstack(all_types_combined)\n",
    "    \n",
    "    print(all_outputs_combined.shape)\n",
    "    print(all_params_combined.shape)\n",
    "    print(all_types_combined.shape)\n",
    "\n",
    "    print(output_path)\n",
    "    \n",
    "    # Concatenate the results, save in 1 file in output_path\n",
    "    np.save(os.path.join(output_path, \"all_outputs.npy\"), all_outputs_combined)\n",
    "    np.save(os.path.join(output_path, 'all_params.npy'), all_params_combined)\n",
    "    np.save(os.path.join(output_path, 'all_types.npy'), all_types_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb7eba",
   "metadata": {},
   "source": [
    "## Process folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca9aed-a91b-4bb6-a116-44fa42c1b3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_info = [(dataset_dir_1, seeding_v_1, par_num)]\n",
    "for folder_name, seeding_v, par_num in tqdm(folder_info): # for each folder\n",
    "    print('Processing ....... ', folder_name)\n",
    "    process_folder(folder_name, seeding_v) # process .mat files by batch and save as temp files\n",
    "    gether_all_data(folder_name) # grab all temp files, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fd7aa-3b7a-4919-bd8a-9d46d5f5abfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate data from each folder, and save as one single dataset\n",
    "gather_final_dataset(folder_info, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3b340-e973-43f9-8f8b-b33796862e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size\n",
    "# Load the files\n",
    "all_outputs = np.load(os.path.join(output_path, 'all_outputs.npy'))\n",
    "all_params  = np.load(os.path.join(output_path, 'all_params.npy'))\n",
    "all_types   = np.load(os.path.join(output_path, 'all_types.npy'))\n",
    "\n",
    "# Check the sizes\n",
    "outputs_shape = all_outputs.shape\n",
    "params_shape  = all_params.shape\n",
    "types_shape   = all_types.shape\n",
    "\n",
    "print(f\"All outputs: {outputs_shape}\")\n",
    "print(f\"All params: {params_shape}\")\n",
    "print(f\"All types:  {types_shape}\")\n",
    "\n",
    "# Normalize profiles\n",
    "norm_outputs = all_outputs / all_outputs.max(axis=2, keepdims=True)\n",
    "filename = os.path.join(output_path, 'all_norm_outputs.npy')\n",
    "np.save(filename, norm_outputs)\n",
    "\n",
    "# Save params in txt format\n",
    "filename = os.path.join(output_path, 'all_params.txt')\n",
    "np.savetxt(filename, all_params, delimiter=',', fmt='%0.8f')\n",
    "print(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
